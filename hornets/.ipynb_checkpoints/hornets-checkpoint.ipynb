{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession, ClientConnectorError\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBase64Image(text):\n",
    "    return isinstance(text, str) and text.startswith('data:image/jpeg;base64')\n",
    "\n",
    "def isUrlImage(text):\n",
    "    return isinstance(text, str) and text.startswith('http')\n",
    "\n",
    "def get_base64_content(initial):\n",
    "    r1 = initial.split('data:image/jpeg;base64,')[1]\n",
    "    return r1\n",
    "\n",
    "def get_image_type(src):\n",
    "    if src == None:\n",
    "        return {'ext': None, 'label': None}\n",
    "    if 'jpeg' in src:\n",
    "        return {'ext': 'jpg', 'label': 'JPEG'}\n",
    "    elif src.startswith('https://'):\n",
    "        return {'ext': 'img', 'label': None}\n",
    "#     else if 'png' in src:\n",
    "#         return {'ext': 'png', 'label': 'PNG'}\n",
    "    else:\n",
    "        return {'ext': None, 'label': None}\n",
    "\n",
    "def get_class_name_from_file(file_name: str):\n",
    "    (class_name, ext) = file_name.split('.html')\n",
    "    return class_name\n",
    "\n",
    "def get_data_dir():\n",
    "    return os.path.join(os.getcwd(), 'data', 'vespa_mandarinia_data')\n",
    "\n",
    "def get_sample_file_name(file_name):\n",
    "    data_dir = get_data_dir\n",
    "    return os.path.join(data_dir, file_name)\n",
    "    \n",
    "async def seed_images():\n",
    "    print('Seeding...')\n",
    "    data_dir = get_data_dir()\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "    os.mkdir(data_dir)\n",
    "    await process_html_files()\n",
    "    print('Seeding complete')\n",
    "\n",
    "    \n",
    "async def stream_to_data(stream):\n",
    "    empty_bytes = b''\n",
    "    result = empty_bytes\n",
    "    while True:\n",
    "        chunk = await stream.read(8)\n",
    "        if chunk == empty_bytes:\n",
    "            break\n",
    "        result += chunk\n",
    "    return result\n",
    "    \n",
    "async def make_file_from_base64(src_string: str, file_name: str):\n",
    "    image_type = get_image_type(src_string)\n",
    "    content = get_base64_content(src_string)\n",
    "    decoded = base64.b64decode(content)\n",
    "    bytes_data = BytesIO(decoded)\n",
    "    image = Image.open(bytes_data)\n",
    "    ext = image_type.get('ext')\n",
    "    file_path = f'{file_name}.{ext}'\n",
    "    image.save(file_path, image_type['label'])\n",
    "    \n",
    "async def make_file_from_url(url: str, file_name: str, session: ClientSession):\n",
    "    try:\n",
    "        resp = await session.request(method=\"GET\", url=url)\n",
    "        image_contents = await stream_to_data(resp.content)\n",
    "        image = Image.open(BytesIO(image_contents))\n",
    "        # TODO: handle other image types\n",
    "        if image.format == 'JPEG':\n",
    "            image.save(file_name + '.jpg', image.format)\n",
    "    except ClientConnectorError:\n",
    "        print('Error getting image: ', url)\n",
    "        \n",
    "async def make_requests(items, **kwargs) -> None:\n",
    "    base64_tasks = []\n",
    "    url_tasks = []\n",
    "\n",
    "    for item in items:\n",
    "        if item['request_type'] == 'base64':\n",
    "            handler = make_file_from_base64(item['src'], item['file_name'])\n",
    "            base64_tasks.append(handler)\n",
    "        elif item['request_type'] == 'url':\n",
    "            url_tasks.append(item)\n",
    "      \n",
    "    base64_results = await asyncio.gather(*base64_tasks)\n",
    "\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for item in url_tasks:\n",
    "            tasks.append(\n",
    "                make_file_from_url(item['src'], item['file_name'], session=session)\n",
    "            )\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "async def process_html_files():\n",
    "    all_items = []\n",
    "    files = ['vespa_mandarinia', 'sphex_ichneumoneus', 'sphecius_speciosus']\n",
    "    for file in files:\n",
    "        # use file name same as class_name\n",
    "        class_name = file\n",
    "        path = os.path.join('data', 'html', f'{file}.txt')\n",
    "        items = process_html_file(path, class_name)\n",
    "        all_items.append(items)\n",
    "    # Flatten list\n",
    "    all_items = [ item for item_group in all_items for item in item_group ]\n",
    "    print('Attempting to create {} images'.format(len(all_items)))\n",
    "    next_items = []\n",
    "    count = 0\n",
    "    for item in all_items:\n",
    "        class_name = item['class_name']\n",
    "        file_name = os.path.join(get_data_dir(), f\"{class_name}-{count}\")\n",
    "        next_items.append({**item, 'file_name': file_name})\n",
    "            \n",
    "        count += 1\n",
    "            \n",
    "    results = await make_requests(next_items)\n",
    "                        \n",
    "    \n",
    "\n",
    "def process_html_file(file_name: str, class_name: str):\n",
    "    with open(file_name, 'r') as file:\n",
    "        soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "    el = soup.find(id=\"islrg\")\n",
    "    images = el.find_all('img')\n",
    "    items = []\n",
    "\n",
    "    for image in images:\n",
    "        request_type = None\n",
    "        if (isBase64Image(image.get('src'))):\n",
    "            request_type = 'base64'\n",
    "        elif isUrlImage(image.get('src')):\n",
    "            request_type = 'url'\n",
    "        item = {\n",
    "            \"src\": image.get('src'),\n",
    "            \"alt\": image.get('alt'),\n",
    "            'class_name': class_name,\n",
    "            \"request_type\": request_type\n",
    "        }\n",
    "        if item['request_type'] != None:\n",
    "            items.append(item)\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeding...\n",
      "Attempting to create 1384 images\n",
      "Seeding complete\n"
     ]
    }
   ],
   "source": [
    "# Seed data\n",
    "await seed_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/vespa_mandarinia'\n",
    "\n",
    "image_datasets = []\n",
    "dataloaders = []\n",
    "dataset_sizes = []\n",
    "for x in ['train', 'val']:\n",
    "    image_datasets.push({x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])})\n",
    "    dataloaders.append({x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)})\n",
    "    dataset_sizes.append({x: len(image_datasets[x])})\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
